# 意图分类系统使用说明

## 1. 项目结构
```
project/
├── data/
│   ├── raw/        # 原始数据
│   └── processed/  # 预处理后的数据
├── models/         # 训练好的模型
├── src/
│   ├── preprocessing/
│   │   ├── cleaner.py      # 文本清洗
│   │   └── intent_analyzer.py  # 意图分析
│   ├── models/
│   │   └── train.py        # 模型训练
│   └── utils/
│       └── file_utils.py   # 文件处理工具
├── main.py        # 训练主程序
├──models--bert-base-chinese/  #BERT预训练模型
└── preprocess.py  # 预处理主程序
```

## 2. 环境准备
```bash
pip install pandas numpy torch transformers scikit-learn openpyxl jieba tqdm
```

## 3. 使用步骤

### 3.1 数据准备
1. 将原始Excel文件（语料.xlsx）放在 `data/raw/` 目录下
2. Excel文件第一列应该是文本内容

### 3.2 数据预处理
运行预处理脚本：
```bash
python preprocess.py
```
这将：
- 清洗文本数据（去除无效字符、表情等）
- 联网调用api，使用GPT进行意图标注
- 生成处理后的数据集在 `data/processed/` 目录

### 3.3 模型训练
运行训练脚本：
```bash
python main.py
```
这将：
- 加载预处理后的数据
- 进行数据增强和平衡
- 训练BERT模型
- 保存最佳模型到 `models/` 目录

## 4. 主要文件功能说明

### 4.1 文本清洗 (cleaner.py)
- 去除HTML标签、URL
- 去除无意义的重复字符
- 保留数字和联系方式
- 去除纯符号行

### 4.2 意图分析 (intent_analyzer.py)
- 使用GPT-3.5进行意图标注
- 支持20种意图类别
- 包含重试机制和错误处理

### 4.3 模型训练 (train.py)
使用了BERT预训练模型微调的方法

包含以下优化特性：

- 数据增强：对样本少的类别进行增强
- 类别平衡：使用带权重的采样器
- 早停机制：防止过拟合
- 学习率调度：使用OneCycleLR
- 梯度裁剪：防止梯度爆炸

## 5. 模型使用

### 5.1 预测单条文本示例
```python
from src.models.train import predict_intent
from transformers import BertTokenizer, BertForSequenceClassification
import json

# 加载模型和tokenizer
model_dir = 'models/best_model'
model = BertForSequenceClassification.from_pretrained(model_dir)
tokenizer = BertTokenizer.from_pretrained(model_dir)

# 加载标签映射
with open('models/label_mapping.json', 'r', encoding='utf-8') as f:
    mapping = json.load(f)
id_to_label = {int(k): v for k, v in mapping['id_to_label'].items()}

# 预测文本
text = "你好，我想问一下还款方式"
result = predict_intent(text, model, tokenizer, id_to_label)
print(f"预测意图: {result}")
```

# 意图分类系统使用说明

## 5.2 批量预测文本

### 5.2.1 基本用法
批量预测多条文本的意图，示例代码如下：

```python
from transformers import BertTokenizer, BertForSequenceClassification
import torch
import json
import pandas as pd

# 1. 加载模型和tokenizer
model_dir = 'models/best_model'
model = BertForSequenceClassification.from_pretrained(model_dir)
tokenizer = BertTokenizer.from_pretrained(model_dir)

# 2. 加载标签映射
with open('models/label_mapping.json', 'r', encoding='utf-8') as f:
    mapping = json.load(f)
id_to_label = {int(k): v for k, v in mapping['id_to_label'].items()}

# 3. 准备要预测的文本
texts = [
    "我的微信号是wx123456",
    "欠了十万块钱实在还不起",
    "明天下午三点联系",
    "能不能分期还款"
]

# 4. 批量预测
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
model.eval()

results = []
with torch.no_grad():
    for text in texts:
        encoding = tokenizer(
            text,
            add_special_tokens=True,
            max_length=128,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        input_ids = encoding['input_ids'].to(device)
        attention_mask = encoding['attention_mask'].to(device)
        
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        pred = torch.argmax(outputs.logits, dim=1)
        
        intent = id_to_label[pred.item()]
        results.append({'text': text, 'intent': intent})

# 5. 转换为DataFrame并保存结果
df_results = pd.DataFrame(results)
df_results.to_csv('prediction_results.csv', index=False, encoding='utf-8-sig')
```

### 5.2.2 从CSV文件批量预测

```python
# 1. 读取CSV文件中的文本
df = pd.read_csv('your_texts.csv', encoding='utf-8-sig')
texts = df['text'].values.tolist()  # 假设文本列名为'text'

# 2. 进行预测
# ... [使用上面的代码进行预测] ...
```

## 6. 注意事项

1. 确保有足够的GPU内存
2. 预处理时需要稳定的网络连接（用于GPT API）
3. 模型训练可能需要较长时间，请耐心等待
4. 模型训练过程会自动使用GPU（如果可用）

## 7. 性能优化建议
1. 增加训练轮数：修改train.py中的epochs参数（默认15）
2. 调整学习率：修改max_lr参数（默认2e-5）
3. 调整早停参数：修改patience参数（默认4）
4. 增加数据增强的样本数：修改min_samples参数（默认80）